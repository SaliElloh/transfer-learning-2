{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import wget\n",
    "from zipfile import PyZipFile\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import img_to_array\n",
    "import cv2\n",
    "import h5py\n",
    "import glob\n",
    "import keras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "74 driving sessions\n",
    "\n",
    "each folder contains:\n",
    "    1. video_garmin = roof-mounted_camera, 7500 frames, 1080p, 25fps, video of the driving environment\n",
    "\n",
    "    2. video_saliency.avi: Ground truth for the current run, computed as explained in the paper.b Synchronized with the roof-mounted camera, 7500 frames. saliency maps of the driver attention\n",
    "\n",
    "    3. video_etg.avi: Video recorded from the Eye-Tracking Glasses (ETG). 9000 frames, 720p, 30fps.\t, first perspective of where the driver is looking\n",
    "\n",
    "\t4. etg_samples.txt This file contains the raw fixations of the driver, recorded from the eye-tracking glasses at 60Hz.\n",
    "\t\tThe file is organized as follows:\n",
    "\t\t\ta. #frame_etg | : The first column gives the index of the video_etg frame corresponding to the current gaze data.\n",
    "            b. #frame_gar |: The second column gives the index of the video_garmin frame corresponding to the current gaze data.\n",
    "            c. Xpx |:\n",
    "            d. Ypx | : X and Y columns are the raw gaze coordinates in the video_etg frame.\n",
    "            e. event_type (| code): The fifth column indicates the type of observation, i.e. Fixation or Saccade. (Last column is an internal timestamp, residuum of the data export process. It should be of no use.)\n",
    "\t\t\n",
    "\n",
    "\t5. speed_course_coord.txt: This file contains information on speed, course, latitude, longitude for the current run.\n",
    "\t\tThe file is organized as follows:\n",
    "\t\t#frame | \n",
    "        speed | Speed (km/h) and Course (degree w.r.t North, relative to the north) information are available for each frame of the Garmin video (7500 rows = 5min * 25hz).\n",
    "        course | \n",
    "        lat | \n",
    "        lon: Lat and Lon information are available approximately every 25 frames (e.g. once per second).\n",
    "\t\t\n",
    "\t6. mean_frame.png:\n",
    "\t\tPre-computed mean frame of the garmin video for the current run, average of where the driver was focusing his attention\n",
    "\t\t\n",
    "\t7. mean_gt.png:\n",
    "\t\tPre-computed mean ground truth image for the current run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the directories\n",
    "main_dir = 'C:/Users/selloh/Desktop/Datasets/DREYEVE_DATA.zip'\n",
    "\n",
    "design = main_dir + \"/\" + \"dr(eye)ve_design.txt\"\n",
    "subsequences = main_dir + '/' + 'subsequences.txt'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/selloh/Desktop/Datasets/DREYEVE_DATA.zip/subsequences.txt\n"
     ]
    }
   ],
   "source": [
    "print(subsequences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize and Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate and Improve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
